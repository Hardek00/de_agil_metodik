# Apache Spark - Praktisk Introduktion

## Vad √§r Apache Spark?

Apache Spark √§r ett **distribuerat ber√§kningsramverk** som l√•ter dig bearbeta stora datam√§ngder snabbt genom att sprida arbetet √∂ver flera CPU-k√§rnor eller maskiner.

### Enkelt f√∂rklarat:
```
Pandas: 1 CPU-k√§rna bearbetar data sekventiellt
Spark: Alla CPU-k√§rnor bearbetar data parallellt

Din laptop: 8 CPU-k√§rnor
Pandas: Anv√§nder 1 k√§rna ‚Üí 100% av 1 k√§rna = 12.5% total anv√§ndning
Spark: Anv√§nder 8 k√§rnor ‚Üí 100% av 8 k√§rnor = 100% total anv√§ndning
```

**Resultat**: 5-10x snabbare bearbetning p√• samma maskin!

---

## Varf√∂r Spark vs Era Befintliga Verktyg?

### Pandas vs Spark - Praktisk J√§mf√∂relse

```python
# Pandas (sekventiell)
import pandas as pd
df = pd.read_csv('large_file.csv')  # 2GB ‚Üí 5 minuter, anv√§nder 1 CPU
result = df.groupby('category').sum()  # L√•ngsamt

# PySpark (parallell)
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("MyApp").getOrCreate()
df = spark.read.csv('large_file.csv')  # 2GB ‚Üí 1 minut, anv√§nder alla CPUs
result = df.groupBy('category').sum()  # Mycket snabbare
```

### Prestandaj√§mf√∂relse (typiska siffror):
```
Datafil: 5GB CSV

Pandas:
- Laddning: 15 minuter
- Bearbetning: 25 minuter  
- Total: 40 minuter
- Minne: Kraschar ofta (beh√∂ver >8GB RAM)

PySpark:
- Laddning: 3 minuter
- Bearbetning: 5 minuter
- Total: 8 minuter  
- Minne: Fungerar med 4GB RAM (str√∂mmande processing)
```

---

## Spark i Era Projekt - Konkreta Anv√§ndningsfall

### 1. E-handelsanalys (stora dataset)
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Starta Spark
spark = SparkSession.builder \
    .appName("ECommerceAnalysis") \
    .config("spark.executor.memory", "2g") \
    .getOrCreate()

# L√§s stora e-handelsfiler
orders = spark.read.csv('orders_2023.csv', header=True, inferSchema=True)
customers = spark.read.csv('customers.csv', header=True, inferSchema=True)

# Snabb analys p√• stora data
customer_metrics = orders.join(customers, 'customer_id') \
    .groupBy('customer_id', 'customer_name') \
    .agg(
        sum('total_amount').alias('lifetime_value'),
        count('order_id').alias('total_orders'),
        avg('total_amount').alias('avg_order_value')
    )

# Skriv resultat till BigQuery
customer_metrics.write \
    .format("bigquery") \
    .option("table", "myproject.mydataset.customer_analysis") \
    .mode("overwrite") \
    .save()
```

### 2. Log-analys (m√•nga filer)
```python
# L√§s alla log-filer p√• en g√•ng (tusentals filer)
logs = spark.read.text('logs/2023/*/access.log')

# Parsa och analysera
parsed_logs = logs.select(
    regexp_extract('value', r'^(\S+)', 1).alias('ip'),
    regexp_extract('value', r'"\w+ (\S+)', 1).alias('url'),
    regexp_extract('value', r'" (\d{3})', 1).cast('int').alias('status')
)

# Hitta popul√§raste sidor
popular_pages = parsed_logs \
    .filter(col('status') == 200) \
    .groupBy('url') \
    .count() \
    .orderBy(desc('count')) \
    .limit(10)

popular_pages.show()
```

### 3. Sensor-data aggregering
```python
# IoT sensor data - miljontals readings
sensor_data = spark.read.json('sensor_readings/*.json')

# Aggregera per timme och sensor
hourly_stats = sensor_data \
    .withColumn('hour', date_trunc('hour', 'timestamp')) \
    .groupBy('sensor_id', 'hour') \
    .agg(
        avg('temperature').alias('avg_temp'),
        max('temperature').alias('max_temp'),
        min('temperature').alias('min_temp'),
        count('*').alias('reading_count')
    )

# Filtrera anomalier
anomalies = hourly_stats.filter(
    (col('max_temp') > 50) | (col('min_temp') < -10)
)

anomalies.show()
```

---

## Installation och Setup

### Lokalt p√• Era Maskiner

#### Via pip (enklaste):
```bash
pip install pyspark

# Med BigQuery-support
pip install pyspark[sql]
pip install google-cloud-bigquery-storage
```

#### Test-installation:
```python
from pyspark.sql import SparkSession

# Skapa Spark session
spark = SparkSession.builder \
    .appName("TestApp") \
    .master("local[*]") \
    .getOrCreate()

# Test med enkel data
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
columns = ["name", "age"]

df = spark.createDataFrame(data, columns)
df.show()

# St√§ng Spark
spark.stop()
```

### Docker Setup f√∂r Era Projekt

#### Dockerfile:
```dockerfile
FROM python:3.9-slim

# Installera Java (kr√§vs f√∂r Spark)
RUN apt-get update && apt-get install -y openjdk-11-jre-headless

# Installera Python-paket
COPY requirements.txt .
RUN pip install -r requirements.txt

# Kopiera kod
COPY . /app
WORKDIR /app

# S√§tt milj√∂variabler
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV SPARK_HOME=/usr/local/lib/python3.9/site-packages/pyspark

CMD ["python", "spark_pipeline.py"]
```

#### docker-compose.yml:
```yaml
version: '3.8'

services:
  spark-app:
    build: .
    environment:
      - SPARK_MASTER=local[*]
      - GOOGLE_APPLICATION_CREDENTIALS=/app/service-account.json
    volumes:
      - ./data:/app/data
      - ./service-account.json:/app/service-account.json:ro
    ports:
      - "4040:4040"  # Spark Web UI
```

---

## PySpark API - Grunderna

### DataFrames (som pandas, men distribuerat)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder.appName("Learning").getOrCreate()

# Skapa DataFrame fr√•n data
data = [
    ("Alice", 25, "Engineer", 75000),
    ("Bob", 30, "Manager", 85000),
    ("Charlie", 35, "Engineer", 70000),
    ("Diana", 28, "Analyst", 65000)
]
columns = ["name", "age", "job", "salary"]
df = spark.createDataFrame(data, columns)

# Basic operations (som pandas)
df.show()                           # Visa data
df.printSchema()                    # Schema info
df.describe().show()                # Statistik

# Filtering (som pandas)
engineers = df.filter(col("job") == "Engineer")
high_earners = df.filter(col("salary") > 70000)

# Grouping (som pandas)
avg_salary_by_job = df.groupBy("job").avg("salary")
avg_salary_by_job.show()

# Sorting
df.orderBy(desc("salary")).show()

# Ny kolumn
df_with_bonus = df.withColumn("bonus", col("salary") * 0.1)
df_with_bonus.show()
```

### L√§sa Olika Filformat

```python
# CSV
df_csv = spark.read.csv('data.csv', header=True, inferSchema=True)

# JSON
df_json = spark.read.json('data.json')

# Parquet (mest effektiv)
df_parquet = spark.read.parquet('data.parquet')

# Flera filer samtidigt
df_multiple = spark.read.csv('data/2023/*/sales.csv', header=True)

# Med schema (snabbare √§n inferSchema)
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True),
    StructField("salary", IntegerType(), True)
])

df_with_schema = spark.read.csv('data.csv', header=True, schema=schema)
```

---

## Integration med BigQuery

### L√§sa fr√•n BigQuery
```python
# Konfigurera BigQuery
spark.conf.set("viewsEnabled", "true")
spark.conf.set("materializationDataset", "temp_dataset")

# L√§s direkt fr√•n BigQuery
df = spark.read \
    .format("bigquery") \
    .option("table", "myproject.mydataset.mytable") \
    .load()

# Eller med SQL
df = spark.read \
    .format("bigquery") \
    .option("query", """
        SELECT customer_id, SUM(amount) as total
        FROM `myproject.mydataset.orders` 
        WHERE order_date >= '2023-01-01'
        GROUP BY customer_id
    """) \
    .load()
```

### Skriva till BigQuery
```python
# Skriv resultat tillbaka
result_df.write \
    .format("bigquery") \
    .option("table", "myproject.mydataset.results") \
    .option("writeMethod", "direct") \
    .mode("overwrite") \
    .save()

# Append mode f√∂r att l√§gga till data
daily_summary.write \
    .format("bigquery") \
    .option("table", "myproject.mydataset.daily_stats") \
    .mode("append") \
    .save()
```

---

## Spark Web UI - Monitoring

N√§r ni k√∂r Spark lokalt kan ni √∂vervaka prestanda:

```
√ñppna browser: http://localhost:4040

Flikar att kolla:
- Jobs: Se vilka operationer som k√∂rs
- Stages: Detaljerat f√∂r varje bearbetningssteg  
- Storage: Cachade DataFrames
- Executors: CPU och minne-anv√§ndning
```

### Exempel p√• vad ni ser:
```
Jobs Tab:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Job ID ‚îÇ Description     ‚îÇ Duration ‚îÇ Status   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 0      ‚îÇ csv read        ‚îÇ 2.3s     ‚îÇ Success  ‚îÇ
‚îÇ 1      ‚îÇ groupBy         ‚îÇ 1.8s     ‚îÇ Running  ‚îÇ
‚îÇ 2      ‚îÇ write to BQ     ‚îÇ -        ‚îÇ Pending  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Optimering och Best Practices

### 1. Caching f√∂r √Öteranv√§ndning
```python
# Om ni anv√§nder samma DataFrame flera g√•nger
large_df = spark.read.csv('huge_file.csv')

# Cache i minnet f√∂r snabbare access
large_df.cache()

# Nu √§r alla operationer snabbare
result1 = large_df.filter(col('amount') > 1000).count()
result2 = large_df.groupBy('category').sum('amount')  # Anv√§nder cachad data
result3 = large_df.select('customer_id').distinct()   # Ocks√• snabbare
```

### 2. Partitionering f√∂r Prestanda
```python
# Partitionera data f√∂r parallell processing
df.write \
    .partitionBy('year', 'month') \
    .parquet('output/partitioned_data')

# N√§r ni l√§ser tillbaka - mycket snabbare f√∂r datum-queries
df = spark.read.parquet('output/partitioned_data')
jan_data = df.filter((col('year') == 2023) & (col('month') == 1))
```

### 3. R√§tt Antal Partitioner
```python
# Kolla nuvarande partitioner
print(f"Partitions: {df.rdd.getNumPartitions()}")

# Optimera f√∂r er maskin (vanligtvis 2-4 per CPU-k√§rna)
df_optimized = df.repartition(16)  # F√∂r 8-k√§rnig maskin

# Eller baserat p√• kolumn
df_by_category = df.repartition('category')
```

---

## Vanliga Fel och L√∂sningar

### 1. Minnes-problem
```python
# Problem: "Java heap space" error
# L√∂sning: √ñka minne
spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "2g") \
    .getOrCreate()
```

### 2. L√•ngsamma Joins
```python
# Problem: L√•ngsam join
large_df.join(small_df, 'key')  # L√•ngsam

# L√∂sning: Broadcast join f√∂r sm√• tables
from pyspark.sql.functions import broadcast
large_df.join(broadcast(small_df), 'key')  # Mycket snabbare
```

### 3. F√∂r m√•nga sm√• filer
```python
# Problem: Tusentals sm√• filer = l√•ngsam
df.write.csv('output/')  # Skapar m√•nga sm√• filer

# L√∂sning: Coalesce f√∂re skrivning
df.coalesce(10).write.csv('output/')  # 10 filer ist√§llet
```

---

## Praktiska Projekt f√∂r Era Studenter

### Projekt 1: E-handelsanalys
```python
"""
M√•l: Analysera f√∂rs√§ljningsdata f√∂r att hitta trends

Data: orders.csv (1-5GB), customers.csv, products.csv
Uppgifter:
1. L√§s in alla filer med Spark
2. Skapa customer lifetime value
3. Hitta popul√§raste produkter per m√•nad
4. Identifiera s√§songseffekter
5. Exportera resultat till BigQuery
"""

def ecommerce_analysis():
    # Era implementationer h√§r
    orders = spark.read.csv('orders.csv', header=True, inferSchema=True)
    # ... forts√§tt analysen
```

### Projekt 2: Log-analys
```python
"""
M√•l: Analysera webbserver-loggar f√∂r att f√∂rst√• anv√§ndarbeteende

Data: access.log filer (m√•nga GB)
Uppgifter:
1. Parsa log-format med regex
2. Hitta popul√§raste sidor
3. Identifiera fel-m√∂nster (404, 500)
4. Analysera trafik per timme
5. Skapa dashboard-data
"""

def log_analysis():
    logs = spark.read.text('logs/*.log')
    # Era parsing och analys h√§r
```

### Projekt 3: IoT sensor-data
```python
"""
M√•l: Bearbeta sensor-data f√∂r anomali-detektion

Data: JSON-filer fr√•n sensorer (m√•nga sm√• filer)
Uppgifter:
1. L√§s tusentals JSON-filer effektivt
2. Aggregera data per timme/dag
3. Hitta temperatur-anomalier
4. Ber√§kna rullande medelv√§rden
5. Spara aggregerad data f√∂r dashboard
"""

def sensor_analysis():
    sensor_data = spark.read.json('sensors/*/*.json')
    # Era analytics h√§r
```

---

## Spark vs Andra Verktyg - N√§r Anv√§nda Vad?

### Beslutsmatris:
```
Datasize < 1GB:
‚úÖ Pandas: Snabbt, enkelt, l√§tt att debugga
‚ùå Spark: Overkill, overhead f√∂r setup

Datasize 1-10GB:
‚úÖ Spark: Mycket snabbare √§n pandas
‚úÖ BigQuery: Om data redan finns d√§r
‚ùå Pandas: Risk f√∂r memory crash

Datasize > 10GB:
‚úÖ Spark: Perfekt anv√§ndningsomr√•de
‚úÖ Dataflow: F√∂r managed processing
‚ùå Pandas: Fungerar inte

Realtidsdata:
‚úÖ Spark Streaming: Kontinuerlig processing
‚úÖ Dataflow: Managed streaming
‚ùå Pandas: Inte designat f√∂r streaming

Team med SQL-fokus:
‚úÖ BigQuery + dbt: SQL-baserat
‚úÖ Spark SQL: SQL p√• stora data
‚ùå Pandas: Kr√§ver Python-kunskap

Komplex ML:
‚úÖ Spark MLlib: Distribuerad ML
‚úÖ Vertex AI: Managed ML
‚ùå Pandas + scikit-learn: Begr√§nsat av minne
```

---

## Installation Guide f√∂r Era Projekt

### Steg 1: Basic Setup
```bash
# Skapa projekt-milj√∂
python -m venv spark_env
source spark_env/bin/activate  # Linux/Mac
# eller
spark_env\Scripts\activate     # Windows

# Installera
pip install pyspark pandas google-cloud-bigquery-storage
```

### Steg 2: Test-k√∂rning
```python
# test_spark.py
from pyspark.sql import SparkSession
import time

start_time = time.time()

spark = SparkSession.builder \
    .appName("SparkTest") \
    .master("local[*]") \
    .getOrCreate()

# Skapa test-data
data = [(i, f"user_{i}", i * 10) for i in range(1000000)]
df = spark.createDataFrame(data, ["id", "name", "score"])

# Testoperationer
result = df.filter(df.score > 5000).groupBy("name").sum("score")
count = result.count()

print(f"Processed {count} records in {time.time() - start_time:.2f} seconds")
spark.stop()
```

### Steg 3: BigQuery Integration
```python
# bigquery_test.py
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("BigQueryTest") \
    .config("spark.jars.packages", "com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.32.2") \
    .getOrCreate()

# Test l√§sning fr√•n BigQuery
df = spark.read \
    .format("bigquery") \
    .option("table", "bigquery-public-data.samples.shakespeare") \
    .load()

df.show(5)
spark.stop()
```

---

## Sammanfattning f√∂r Era 9 Veckor

### Vad √§r Spark?
- **Distribuerad ber√§kning** - anv√§nder alla CPU-k√§rnor parallellt
- **PySpark** - Python API som k√§nns som pandas
- **Skalbar** - fr√•n lokal maskin till stora kluster
- **Snabb** - 5-10x snabbare √§n pandas f√∂r stora data

### Varf√∂r Relevant f√∂r Er?
- **P√•taglig f√∂rb√§ttring** - ni ser direkt skillnad i prestanda
- **Praktiskt anv√§ndbart** - kan faktiskt anv√§nda i era projekt
- **Industri-standard** - anv√§nds √∂verallt f√∂r big data
- **Bygger p√• Python** - anv√§nder befintlig kunskap

### Vad Ni Kan G√∂ra:
1. **Installera och testa** - f√• det att fungera lokalt
2. **Konvertera pandas-kod** - samma logik, snabbare exekvering
3. **Integrera med BigQuery** - l√§sa/skriva stora dataset
4. **Anv√§nd i grupprojekt** - bearbeta st√∂rre data tillsammans

### Realistic Timeline:
- **Vecka 1**: Installation och basic DataFrame-operationer
- **Vecka 2**: L√§sa filer och enkla aggregeringar  
- **Vecka 3**: BigQuery-integration och f√∂rsta "riktiga" projekt
- **Resten**: Anv√§nda Spark f√∂r era data-pipelines

**Bottom line**: Spark √§r det verktyg som faktiskt kan f√∂rb√§ttra era nuvarande projekt direkt. Det √§r som pandas fast snabbare - perfekt f√∂r att k√§nna kraften i distribuerad ber√§kning utan att beh√∂va l√§ra sig helt nya koncept!

Nu kan ni faktiskt bearbeta de d√§r 5GB CSV-filerna som pandas kraschar p√•! üöÄ
